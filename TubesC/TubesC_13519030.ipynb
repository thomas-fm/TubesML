{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00800ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bdf4859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer0\n",
      "[[-0.32353658  0.22715959  0.32790573]\n",
      " [ 0.71939066  0.26399003 -0.03843606]\n",
      " [-0.48745541 -1.28216133  0.14723296]\n",
      " [-0.19426756 -1.25200704 -0.69835966]]\n",
      "Layer1\n",
      "[[ 0.36669884  0.77152883]\n",
      " [-0.61513406  2.14831828]\n",
      " [ 0.27901717 -0.640975  ]]\n",
      "Layer2\n",
      "[[-0.27136608  0.8348465   0.22330486 -0.16273927 -0.00367248]\n",
      " [-0.48156779  1.36845964  1.33401936 -1.14327385  1.07250144]]\n",
      "Layer3\n",
      "[[-0.00847987 -0.5097879  -0.6916257 ]\n",
      " [ 1.69847043 -0.9377576  -0.65542002]\n",
      " [-0.49086006  1.04463058 -0.95538969]\n",
      " [-1.04559569 -1.40341864  1.36967141]\n",
      " [ 0.15302076  0.6056277  -0.76876388]]\n",
      "Jumlah iterasi\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "iris = load_iris()\n",
    "datasets = train_test_split(iris.data, iris.target,test_size=0.2)\n",
    "train_data, test_data, train_labels, test_labels = datasets\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', hidden_layer_sizes=(3, 2, 5), max_iter=100, batch_size=2)\n",
    "clf.fit(train_data, train_labels)   \n",
    "clf.score(train_data, train_labels)\n",
    "\n",
    "# Bobot layer\n",
    "i = 0\n",
    "for w in clf.coefs_:\n",
    "    print(f\"Layer{i}\")\n",
    "    print(w)\n",
    "    i+=1\n",
    "\n",
    "print(\"Jumlah iterasi\")\n",
    "print(clf.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4817c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import seed\n",
    "from random import random, uniform\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import graphviz\n",
    "import copy\n",
    "from random import randint\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "class Data:\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "        self.target = []\n",
    "        self.target_names = []\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    net_h = np.array(x)\n",
    "    numerator = np.exp(net_h)\n",
    "    denominator = np.sum(np.exp(x))\n",
    "    softmax_output = numerator / denominator\n",
    "    return softmax_output\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return 1\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return x * (1 - x)\n",
    "\n",
    "# todo\n",
    "def relu_derivative(x):\n",
    "    return [0 if (el < 0) else 1 for el in x]\n",
    "\n",
    "# todo\n",
    "def softmax_derivative(x):\n",
    "    return [-(1-el) for el in x]\n",
    "\n",
    "def split_dataset_90_10(dataset):\n",
    "    data_length = len(dataset.target)\n",
    "    n_train = round(data_length * 9/10)\n",
    "    n_test = data_length - n_train\n",
    "    \n",
    "    train = Data()\n",
    "    test = Data()\n",
    "    train.target_names = dataset.target_names\n",
    "    test.target_names = dataset.target_names\n",
    "\n",
    "    test_idx = []\n",
    "    while len(test_idx) < n_test:\n",
    "        idx = randint(0, 149)\n",
    "        try:\n",
    "            test_idx.index(idx)\n",
    "        except:\n",
    "            test_idx.append(idx)\n",
    "            test.data.append(dataset.data[idx])\n",
    "            test.target.append(dataset.target[idx])\n",
    "            \n",
    "    for i in range(data_length):\n",
    "        try:\n",
    "            test_idx.index(i)\n",
    "        except:\n",
    "            train.data.append(dataset.data[i])\n",
    "            train.target.append(dataset.target[i])\n",
    "    return train, test\n",
    "\n",
    "def confusionMatrix(y_test, y_pred):\n",
    "    x = len(set(y_test))\n",
    "    confusion_matrix = [[0 for i in range(x)] for j in range(x)]\n",
    "    for i in range(len(y_test)):\n",
    "        confusion_matrix[y_test[i]][y_pred[i]] += 1\n",
    "    return np.array(confusion_matrix)\n",
    "\n",
    "def accuracy(confusion_matrix):\n",
    "    np.seterr(invalid='ignore')\n",
    "    return np.nan_to_num(np.sum(np.diag(confusion_matrix)) / np.sum(confusion_matrix))\n",
    "\n",
    "def precision(confusion_matrix):\n",
    "    np.seterr(invalid='ignore')\n",
    "    return np.nan_to_num(np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=0))\n",
    "\n",
    "def recall(confusion_matrix):\n",
    "    np.seterr(invalid='ignore')\n",
    "    return np.nan_to_num(np.diag(confusion_matrix) / np.sum(confusion_matrix, axis=1))\n",
    "\n",
    "def f1(confusion_matrix):\n",
    "    np.seterr(invalid='ignore')\n",
    "    return np.nan_to_num(2 * precision(confusion_matrix) * recall(confusion_matrix) / (precision(confusion_matrix) + recall(confusion_matrix)))\n",
    "\n",
    "def summary(confusion_matrix):\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix)\n",
    "    print(\"Accuracy:\", accuracy(confusion_matrix))\n",
    "    print(\"Precision:\", precision(confusion_matrix))\n",
    "    print(\"Recall:\", recall(confusion_matrix))\n",
    "    print(\"F1:\", f1(confusion_matrix))\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_input, n_nodes):\n",
    "        self.weights = []\n",
    "        self.n_input = n_input\n",
    "        self.n_nodes = n_nodes\n",
    "        self.activations = \"\"\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        self.error = []\n",
    "        self.updated_weights = []\n",
    "\n",
    "    def update_weights_back_propagation(self):\n",
    "        self.weights = self.updated_weights.copy()\n",
    "        self.updated_weights = []\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, n_layers, n_neuron=[], activation=[],\n",
    "                 learning_rate=0.1, err_threshold=0.01,\n",
    "                 max_iter=100, batch_size=2, dataset=load_iris(),\n",
    "                 n_input=4, n_output=3):\n",
    "        # Load iris dataset\n",
    "        self.dataset = dataset  # dataset\n",
    "        self.input = dataset.data  # input\n",
    "        self.target = dataset.target  # target\n",
    "        self.target_names = dataset.target_names  # target class name\n",
    "        self.n_attr = n_input  # n input attribute\n",
    "\n",
    "        # Neural network\n",
    "        self.n_layers = n_layers  # how many hidden layers\n",
    "        self.n_neuron = n_neuron  # how many neuron for each hidden layer\n",
    "        self.activation = activation  # activation for each layer\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.err_threshold = err_threshold\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.layers = []\n",
    "        self.bias = 1\n",
    "        self.output = []  # final output from forward propagate\n",
    "        self.mse = 999\n",
    "\n",
    "        # Back prop\n",
    "        self.error_hidden_value = 0\n",
    "        self.updated_weights = []\n",
    "        self.error = 999  # current error?\n",
    "        self.weights = []  # last updated weight\n",
    "        self.predict = []\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            # n input = n neuron in layer\n",
    "            if i == 0:\n",
    "                layer = Layer(self.n_attr+1, n_neuron[i])\n",
    "                layer.weights = [\n",
    "                    [uniform(-0.5, 0.5) for i in range(n_neuron[i])] for j in range(self.n_attr+1)]\n",
    "            else:\n",
    "                layer = Layer(n_neuron[i-1]+1, n_neuron[i])\n",
    "                layer.weights = [\n",
    "                    [uniform(-0.5, 0.5) for i in range(n_neuron[i])] for j in range(n_neuron[i-1]+1)]\n",
    "            # initalize weight\n",
    "            layer.activations = activation[i]\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # add last layer, last hidden to output\n",
    "        layer = Layer(n_neuron[-1] + 1, n_output)\n",
    "        layer.weights = [[uniform(-0.5, 0.5) for i in range(n_output)]\n",
    "                         for j in range(n_neuron[-1] + 1)]\n",
    "        layer.activations = \"softmax\"\n",
    "        self.layers.append(layer)\n",
    "\n",
    "        # with open(file_name, \"r\") as f:\n",
    "        #     line = f.readline().split()\n",
    "        #     self.n_layers = len(line) - 1 # how many hidden layers + ouput\n",
    "            \n",
    "        #     for i in range(self.n_layers + 1):\n",
    "        #         if i == 0:\n",
    "        #             self.layers.append(Layer(self.n_attr, int(line[i])))\n",
    "        #         else:\n",
    "        #             self.layers.append(Layer(int(line[i-1]), int(line[i])))\n",
    "\n",
    "        #     for i in range(self.n_layers + 1):\n",
    "        #         f.readline()\n",
    "        #         for j in range(self.layers[i].n_input + 2):\n",
    "        #             weight = []\n",
    "        #             line = f.readline().strip(\" \\n\").split(\" \")\n",
    "        #             for k in range(len(line)):\n",
    "        #                 if (j == 0):\n",
    "        #                     self.layers[i].activations = str(line[k])\n",
    "        #                 else:\n",
    "        #                     weight.append(float(line[k]))\n",
    "        #             if j!=0:\n",
    "        #                 self.layers[i].weights.append(weight)\n",
    "\n",
    "    def load_model(self, file_name):\n",
    "        with open(file_name, \"r\") as f:\n",
    "            line = f.readline().split()\n",
    "            self.n_layers = len(line) - 1 # how many hidden layers + ouput\n",
    "            \n",
    "            for i in range(self.n_layers + 1):\n",
    "                if i == 0:\n",
    "                    self.layers.append(Layer(self.n_attr, int(line[i])))\n",
    "                else:\n",
    "                    self.layers.append(Layer(int(line[i-1]), int(line[i])))\n",
    "\n",
    "            for i in range(self.n_layers + 1):\n",
    "                f.readline()\n",
    "                self.layers[i].weights = []\n",
    "                for j in range(self.layers[i].n_input + 1):\n",
    "                    weight = []\n",
    "                    line = f.readline().strip(\" \\n\").split(\" \")\n",
    "                    \n",
    "                    if len(line[0]) != 0:\n",
    "                        for k in range(len(line)):\n",
    "                            if (j == 0):\n",
    "                                self.layers[i].activations = str(line[k])\n",
    "                            else:\n",
    "                                weight.append(float(line[k]))\n",
    "                        if j!=0:\n",
    "                            self.layers[i].weights.append(weight)\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        new_file = []\n",
    "        with open(filename) as file:\n",
    "            lines = [line.rstrip().split() for line in file]\n",
    "\n",
    "            n_neuron = []\n",
    "            for i in range(self.n_layers + 1):\n",
    "                n_neuron.append(self.layers[i].n_nodes)\n",
    "\n",
    "            new_file.append(n_neuron)\n",
    "            new_file.append('')\n",
    "            \n",
    "            for i in range(self.n_layers + 1):\n",
    "                new_file.append([self.layers[i].activations])\n",
    "                for new_weight in self.layers[i].weights:\n",
    "                    new_file.append(new_weight)\n",
    "                if i < self.n_layers:\n",
    "                    new_file.append('')\n",
    "                    \n",
    "            for line in range(len(new_file)):\n",
    "                str_line = ''\n",
    "                for i in range(len(new_file[line])):\n",
    "                    str_line += str(new_file[line][i])\n",
    "                    if i < len(new_file[line]) - 1:\n",
    "                        str_line += ' '\n",
    "                new_file[line] = str_line\n",
    "                \n",
    "        new_filename = filename.split(\".\")[0] + \"_updated_weights\"\n",
    "        with open(new_filename + '.txt', 'w') as f:\n",
    "            for line in range(len(new_file)):\n",
    "                f.write(new_file[line])\n",
    "                if line < len(new_file) - 1:\n",
    "                    f.write('\\n')\n",
    "\n",
    "    def forward_propagation(self, type):\n",
    "        for i in range(self.n_layers + 1):\n",
    "            self.layers[i].input = []\n",
    "        # The first input layer\n",
    "        if type == \"train\":\n",
    "            self.layers[0].input = self.input\n",
    "        elif type == \"predict\":\n",
    "            self.layers[0].input = self.predict\n",
    "\n",
    "        # All hidden layers\n",
    "        for i in range(self.n_layers + 1):\n",
    "            # add bias\n",
    "            bias_input = self.bias\n",
    "            if (i == 0):  # if first hidden layer, convert to array from ndarray and then add bias in the last index\n",
    "                temp_input = []\n",
    "                for j in range(len(self.layers[i].input)):\n",
    "                    input_row = []\n",
    "                    for k in range(len(self.layers[i].input[j])):\n",
    "                        input_row.append(self.layers[i].input[j][k])\n",
    "                    input_row.append(bias_input)\n",
    "                    temp_input.append(input_row)\n",
    "                self.layers[i].input = temp_input\n",
    "            else:  # if not first layer the immediately add the bias in the last index\n",
    "                for j in range(len(self.layers[i].input)):\n",
    "                    self.layers[i].input[j].append(bias_input)\n",
    "\n",
    "            # calculate sigma\n",
    "            self.layers[i].output = np.dot(\n",
    "                self.layers[i].input, self.layers[i].weights)\n",
    "\n",
    "            # activation function\n",
    "            for j in range(len(self.layers[i].output)):\n",
    "                input_next_layer = []  # temporary list to store the next layer's input\n",
    "                for k in range(len(self.layers[i].output[j])):\n",
    "                    x = self.layers[i].output[j][k]\n",
    "                    result = 0\n",
    "                    if i != self.n_layers:\n",
    "                        if (self.layers[i].activations.lower() == \"linear\"):\n",
    "                            result = format(linear(x))\n",
    "                        elif (self.layers[i].activations.lower() == \"sigmoid\"):\n",
    "                            result = format(sigmoid(x))\n",
    "                        elif (self.layers[i].activations.lower() == \"relu\"):\n",
    "                            result = format(relu(x))\n",
    "                        # elif (self.layers[i].activations.lower() == \"softmax\"):\n",
    "                        #     result = format(softmax(x))\n",
    "                        else:  # if activation is not linier, relu, sigmoid, or softmax\n",
    "                            print(\n",
    "                                f\"{self.layers[i].activations}: Invalid activation method!\")\n",
    "                            return\n",
    "                    else:\n",
    "                        result = format(softmax(x))\n",
    "\n",
    "                    # append output, actually layers[i].output == layers[i+1].input\n",
    "                    self.layers[i].output[j][k] = result\n",
    "                    # append input for next layer in temporary list (input_next_layer)\n",
    "                    input_next_layer.append(float(result))\n",
    "\n",
    "                if (i < self.n_layers):  # if there is still the next layer\n",
    "                    # append input for next layer in layers[i+1].input\n",
    "                    self.layers[i+1].input.append(input_next_layer)\n",
    "\n",
    "        # output in the last layer\n",
    "        self.output = self.layers[-1].output.copy()\n",
    "\n",
    "    # todo\n",
    "    def error_output(self):\n",
    "        # get output layer\n",
    "        mse_sum = 0\n",
    "        total = 0\n",
    "        output_layer = self.layers[self.n_layers]\n",
    "        activation_rule = output_layer.activations.lower()\n",
    "        output_layer.error = output_layer.output.copy()\n",
    "        for i in range(len(self.input)):\n",
    "            expected_target = []\n",
    "            if (self.target[i] == 0):\n",
    "                expected_target = [1, 0, 0]\n",
    "            if (self.target[i] == 1):\n",
    "                expected_target = [0, 1, 0]\n",
    "            if (self.target[i] == 2):\n",
    "                expected_target = [0, 0, 1]\n",
    "            for j in range(3):\n",
    "                output_layer.error[i][j] = expected_target[j] - output_layer.error[i][j]\n",
    "                mse_sum += pow(expected_target[j] - output_layer.error[i][j], 2)\n",
    "                total += 1\n",
    "        self.mse = (1/total) * mse_sum\n",
    "        self.error = np.mean(output_layer.error)\n",
    "\n",
    "        # calculate error per output node\n",
    "        for i in range(len(output_layer.error)):\n",
    "            if (activation_rule == \"sigmoid\"):\n",
    "                output_layer.error[i] *= sigmoid_derivative(\n",
    "                    output_layer.output[i])\n",
    "            elif (activation_rule == \"relu\"):\n",
    "                output_layer.error[i] *= relu_derivative(\n",
    "                    output_layer.output[i])\n",
    "            elif (activation_rule == \"linear\"):\n",
    "                output_layer.error[i] *= linear_derivative(\n",
    "                    output_layer.output[i])\n",
    "            elif (activation_rule == \"softmax\"):\n",
    "                output_layer.error[i] *= softmax_derivative(\n",
    "                    output_layer.output[i])\n",
    "            \n",
    "        return\n",
    "\n",
    "    # todo\n",
    "    def error_hidden(self):\n",
    "        #menghitung delta net j = delta k * weight j k\n",
    "        delta_net_j = 0\n",
    "        \n",
    "        for i in range(self.n_layers-1, -1, -1):\n",
    "            delta_net_j_array = []\n",
    "            # print( self.layers[i].error, \"\\n\", self.layers[i+1].weights[i])\n",
    "            for j in range(len(self.layers[i+1].error)):\n",
    "                delta_net_j_data = []\n",
    "                for k in range(len(self.layers[i+1].weights)-1):\n",
    "                    # print(self.layers[i+1].error)\n",
    "                    delta_net_j = np.dot(self.layers[i+1].error[j], np.transpose(self.layers[i+1].weights[k]))\n",
    "                    delta_net_j_data.append(delta_net_j)\n",
    "                    \n",
    "\n",
    "                delta_net_j_array.append(delta_net_j_data)\n",
    "\n",
    "            self.layers[i].error = self.layers[i].output.copy()\n",
    "\n",
    "            #menghitung error di hidden layer\n",
    "            for j in range(len(self.layers[i].error)):\n",
    "                if (self.layers[i].activations == \"sigmoid\"):\n",
    "                    self.layers[i].error[j] = np.dot(sigmoid_derivative(\n",
    "                        self.layers[i].output[j]),  delta_net_j_array[j])\n",
    "                elif (self.layers[i].activations == \"relu\"):\n",
    "                    self.layers[i].error[j] = np.dot(relu_derivative(\n",
    "                        self.layers[i].output[j]), delta_net_j_array[j])\n",
    "                elif (self.layers[i].activations == \"linear\"):\n",
    "                    self.layers[i].error[j] = np.dot(linear_derivative(\n",
    "                        self.layers[i].output[j]), delta_net_j_array[j])\n",
    "        \n",
    "        return\n",
    "\n",
    "    # todo\n",
    "    def update_weights(self, row_input, old_weight, error_term):\n",
    "        new_weight_list = []\n",
    "\n",
    "        # new weight for hidden layers\n",
    "        for i in range(len(row_input)):\n",
    "            new_weight = old_weight[i] - self.learning_rate * error_term * row_input[i]\n",
    "            new_weight_list.append(new_weight)\n",
    "        \n",
    "        return new_weight_list\n",
    "\n",
    "    # todo\n",
    "    def back_propagation(self):\n",
    "        # output layer\n",
    "        self.error_output()\n",
    "        output_layer = self.layers[self.n_layers]\n",
    "        self.error = np.mean(output_layer.error)\n",
    "        updated_weights_temp = []\n",
    "        \n",
    "        for i in range(len(output_layer.output)):\n",
    "            updated_weights_temp.append(self.update_weights(\n",
    "                output_layer.input[i], output_layer.weights, output_layer.error[i]))\n",
    "        output_layer.updated_weights = output_layer.weights.copy()\n",
    "        output_layer.updated_weights = np.mean(\n",
    "            updated_weights_temp, axis=0)\n",
    "\n",
    "        self.error_hidden()\n",
    "        for i in range(self.n_layers-1, -1 ,-1):\n",
    "            self.error_hidden_value = np.mean(self.layers[i].error)\n",
    "            updated_weights_temp_hidden = []\n",
    "            for j in range(len(self.layers[i].input)):\n",
    "                updated_weights_temp_hidden.append(self.update_weights(self.layers[i].input[j], self.layers[i].weights, self.layers[i].error[j]))\n",
    "            self.layers[i].updated_weights = self.layers[i].weights.copy()\n",
    "            self.layers[i].updated_weights = np.mean(updated_weights_temp_hidden, axis=0)\n",
    "\n",
    "        # update all weights\n",
    "        for layer in self.layers:\n",
    "            layer.update_weights_back_propagation()\n",
    "        return\n",
    "\n",
    "    # todo\n",
    "    def train(self):\n",
    "        it = 0\n",
    "        input_temp = copy.copy(self.input)\n",
    "\n",
    "        while ((it < self.max_iter) and (self.err_threshold < self.mse)):\n",
    "            output = []\n",
    "            error = []\n",
    "            for i in range(int(len(self.input)/self.batch_size)):\n",
    "                idx = i * self.batch_size\n",
    "                self.input = input_temp[idx:idx+self.batch_size]\n",
    "                self.forward_propagation(type=\"train\")\n",
    "                self.back_propagation()\n",
    "                output.append(self.layers[self.n_layers].output)\n",
    "                error.append(self.layers[self.n_layers].error)\n",
    "            it += 1\n",
    "            self.error = self.error\n",
    "            \n",
    "        return\n",
    "\n",
    "    def set_predict(self, input):\n",
    "        self.predict = input\n",
    "\n",
    "    def prediction(self, test_target):\n",
    "        self.forward_propagation(type=\"predict\")\n",
    "        self.convert_output_to_class(test_target)\n",
    "\n",
    "        return self.output\n",
    "        \n",
    "    def prediction2(self):\n",
    "        self.forward_propagation(type=\"predict\")\n",
    "        self.convert_output_to_class_2()\n",
    "        return self.output\n",
    "        \n",
    "    def check_sanity(self):\n",
    "        for layer in self.layers:\n",
    "            print(layer.weights)\n",
    "\n",
    "    def convert_output_to_class(self, test_target):\n",
    "        self.output_predict = test_target.copy()\n",
    "        for i in range(len(self.output)):\n",
    "            if (self.output[i][0] > self.output[i][1]):\n",
    "                self.output_predict[i] = 0 if (self.output[i][0] > self.output[i][2]) else 2\n",
    "            elif (self.output[i][0] < self.output[i][1]):\n",
    "                self.output_predict[i] = 1 if (self.output[i][1] > self.output[i][2]) else 2\n",
    "    \n",
    "    def convert_output_to_class_2(self):\n",
    "        self.output_predict = self.target.copy()\n",
    "        for i in range(len(self.output)):\n",
    "            if (self.output[i][0] > self.output[i][1]):\n",
    "                self.output_predict[i] = 0 if (self.output[i][0] > self.output[i][2]) else 2\n",
    "            elif (self.output[i][0] < self.output[i][1]):\n",
    "                self.output_predict[i] = 1 if (self.output[i][1] > self.output[i][2]) else 2\n",
    "\n",
    "    def cross_validate(self):\n",
    "        # shuffle dataset\n",
    "        label = np.array(self.dataset.data)\n",
    "        target = np.array(self.dataset.target)\n",
    "\n",
    "        indices = np.arange(label.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        label = label[indices]\n",
    "        target = target[indices]\n",
    "\n",
    "        # split into 10\n",
    "        n = len(self.dataset.target)\n",
    "        j = int(np.ceil(n / 10))\n",
    "\n",
    "        total_mse = 0\n",
    "        acc_score = 0\n",
    "        prec_score = 0\n",
    "        f1_score = 0\n",
    "        rec_score = 0\n",
    "\n",
    "        for it in range(10):\n",
    "            data_train_label = copy.copy(label)\n",
    "            data_train_target = copy.copy(target)\n",
    "\n",
    "            data_train_label = np.concatenate((data_train_label[0:it*j], data_train_label[it*j:it*j+j]))\n",
    "            data_train_target = np.concatenate((data_train_target[0:it*j], data_train_target[it*j:it*j+j]))\n",
    "\n",
    "            self.input = data_train_label\n",
    "            self.target = data_train_target\n",
    "\n",
    "            data_test_label = label[it*j:it*j+j]\n",
    "            data_test_target = target[it*j:it*j+j]\n",
    "            self.predict = data_test_label\n",
    "\n",
    "            # train and predict\n",
    "            self.train()\n",
    "\n",
    "            # calculate error\n",
    "            pred = self.prediction2()\n",
    "            self.convert_output_to_class_2()\n",
    "            expec = []\n",
    "\n",
    "            # transform to [x,x,x]\n",
    "            for i in range(len(self.output)):\n",
    "                expected_target = []\n",
    "                # print(data_test_label[i])\n",
    "                if (data_test_target[i] == 0):\n",
    "                    expected_target = [1, 0, 0]\n",
    "                if (data_test_target[i] == 1):\n",
    "                    expected_target = [0, 1, 0]\n",
    "                if (data_test_target[i] == 2):\n",
    "                    expected_target = [0, 0, 1]\n",
    "                expec.append(expected_target)\n",
    "\n",
    "            # calculate the MSE\n",
    "            pred = np.concatenate(pred).ravel()\n",
    "            expec = np.concatenate(expec).ravel()\n",
    "\n",
    "            # calculate confusion matrix\n",
    "            confusion_matrix = confusionMatrix(data_test_target, self.output_predict)\n",
    "            acc_score += accuracy(confusion_matrix)\n",
    "            f1_score += f1(confusion_matrix)\n",
    "            rec_score += recall(confusion_matrix)\n",
    "            prec_score += precision(confusion_matrix)\n",
    "\n",
    "            sum_mse_cv = 0\n",
    "\n",
    "            for i in range(len(pred)):\n",
    "                sum_mse_cv += pow(pred[i] - expec[i], 2)\n",
    "\n",
    "            sum_mse_cv = float(sum_mse_cv/len(pred))\n",
    "            total_mse += sum_mse_cv\n",
    "\n",
    "        mse_cv = float(total_mse / 10)\n",
    "        print(f\"MSE Score: {1 - mse_cv}\")\n",
    "        print(f\"Average accurarcy: {acc_score/10}\")\n",
    "        print(f\"Average precision: {prec_score/10}\")\n",
    "        print(f\"Average F1: {f1_score/10}\")\n",
    "        print(f\"Average recall: {rec_score/10}\")\n",
    "\n",
    "\n",
    "\n",
    "    def draw_model(self):\n",
    "        f = graphviz.Digraph('Feed Forward Neural Network', filename=\"model\")\n",
    "        f.attr('node', shape='circle', width='1.0')\n",
    "        f.edge_attr.update(arrowhead='vee', arrowsize='2')\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            if i == 0:\n",
    "                for j in range(len(self.layers[i].weights)): #count weights\n",
    "                    for k in range(len(self.layers[i].weights[j])): #output node\n",
    "                        if j==0:\n",
    "                            f.edge(f\"bx{j}\", f\"h{i+1}_{k}\", \n",
    "                                f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "                        else:\n",
    "                            f.edge(f\"x{j}\", f\"h{i+1}_{k}\", \n",
    "                                f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "            else:\n",
    "                for j in range(len(self.layers[i].weights)): #count weights\n",
    "                    for k in range(len(self.layers[i].weights[j])): #output node\n",
    "                        if j==0:\n",
    "                            f.edge(f\"bh{i}\", f\"h{i+1}_{k}\", \n",
    "                                f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "                        else:\n",
    "                            f.edge(f\"h{i}_{j-1}\", f\"h{i+1}_{k}\", \n",
    "                                f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "        \n",
    "        print(f.source)\n",
    "        f.render(directory='model').replace('\\\\', '/')\n",
    "\n",
    "    def testForward(self):\n",
    "        self.prediction_forward()\n",
    "    def set_input(self, inp):\n",
    "        self.input = inp\n",
    "    def set_target(self, target):\n",
    "        self.target = target\n",
    "    \n",
    "    def predict_new_data(self, data):\n",
    "        self.forward_propagation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "518d6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "dataset = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9129e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ UJI IMPLEMENTASI MATRIX CONFUSSION =================\n",
      "Confusion Matrix:\n",
      "[[50  0  0]\n",
      " [ 0 50  0]\n",
      " [ 0  0 50]]\n",
      "Accuracy: 1.0\n",
      "Precision: [1. 1. 1.]\n",
      "Recall: [1. 1. 1.]\n",
      "F1: [1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# UJI CONFUSION MATRIX MODEL IMPLEMENTASI\n",
    "print(\"============ UJI IMPLEMENTASI MATRIX CONFUSSION =================\")\n",
    "nn = NeuralNetwork(n_layers=3, dataset=dataset, batch_size=50, n_neuron=[3, 2, 5], activation=[\"sigmoid\", \"sigmoid\", \"sigmoid\"])\n",
    "nn.train()\n",
    "nn.set_predict(dataset.data)\n",
    "nn.prediction2()\n",
    "summary(confusionMatrix(dataset.target, nn.output_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bba9f1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ UJI SKLEARN MATRIX CONFUSION =================\n",
      "[[14  0 36]\n",
      " [ 5 43  2]\n",
      " [15  1 34]]\n",
      "Accuracy: 0.6066666666666667\n",
      "Precision: [0.41176471 0.97727273 0.47222222]\n",
      "Recall: [0.28 0.86 0.68]\n",
      "F1: [0.33333333 0.91489362 0.55737705]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# UJI CONFUSION MATRIX MODEL SKLEARN\n",
    "print(\"============ UJI SKLEARN MATRIX CONFUSION =================\")\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset.data)\n",
    "\n",
    "train_data = scaler.transform(dataset.data)\n",
    "\n",
    "clf = MLPClassifier(solver='adam', hidden_layer_sizes=(3, 2, 5), max_iter=100, batch_size=50)\n",
    "clf.fit(train_data, dataset.target)  \n",
    "\n",
    "print(confusion_matrix(dataset.target, clf.predict(train_data)))\n",
    "print(f\"Accuracy: {accuracy_score(dataset.target, clf.predict(train_data), normalize=False)/float(150)}\")\n",
    "print(f\"Precision: {precision_score(dataset.target, clf.predict(train_data), average=None)}\")\n",
    "print(f\"Recall: {recall_score(dataset.target, clf.predict(train_data), average=None)}\")\n",
    "print(f\"F1: {f1_score(dataset.target, clf.predict(train_data), average=None)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aeb4f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ UJI IMPLEMENTASI SPLIT TEST =================\n",
      "Confusion Matrix:\n",
      "[[3 0 0]\n",
      " [0 5 0]\n",
      " [0 0 7]]\n",
      "Accuracy: 1.0\n",
      "Precision: [1. 1. 1.]\n",
      "Recall: [1. 1. 1.]\n",
      "F1: [1. 1. 1.]\n",
      "============ UJI SKLEARN SPLIT TEST =================\n",
      "[[3 0 0]\n",
      " [0 5 0]\n",
      " [0 4 3]]\n",
      "0.07333333333333333\n",
      "[1.         0.55555556 1.        ]\n",
      "[1.         1.         0.42857143]\n",
      "[1.         0.71428571 0.6       ]\n"
     ]
    }
   ],
   "source": [
    "# UJI SKEMA SPLIT TRAIN\n",
    "print(\"============ UJI IMPLEMENTASI SPLIT TEST =================\")\n",
    "train, test = split_dataset_90_10(dataset)\n",
    "nn = NeuralNetwork(n_layers=3, dataset=train, batch_size=50, n_neuron=[3, 2, 5], activation=[\"sigmoid\", \"sigmoid\", \"sigmoid\"])\n",
    "nn.train()\n",
    "nn.set_predict(test.data)\n",
    "nn.prediction(test.target)\n",
    "summary(confusionMatrix(test.target, nn.output_predict))\n",
    "\n",
    "# sklearn train result\n",
    "print(\"============ UJI SKLEARN SPLIT TEST =================\")\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train.data)\n",
    "train_data = scaler.transform(train.data)\n",
    "scaler.fit(test.data)\n",
    "test_data = scaler.transform(test.data)\n",
    "\n",
    "clf = MLPClassifier(solver='adam', hidden_layer_sizes=(3, 2, 5), max_iter=1000, batch_size=50)\n",
    "clf.fit(train_data, train.target)\n",
    "target_pred = clf.predict(test_data)\n",
    "print(confusion_matrix(test.target, target_pred))\n",
    "print(accuracy_score(test.target, target_pred, normalize=False)/float(150))\n",
    "print(precision_score(test.target, target_pred, average=None))\n",
    "print(recall_score(test.target, target_pred, average=None))\n",
    "print(f1_score(test.target, target_pred, average=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55f7109e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ UJI IMPLEMENTASI CROSS VALIDATION =================\n",
      "MSE Score: 0.33333333333333326\n",
      "Average accurarcy: 0.42666666666666675\n",
      "Average precision: [0.4        0.4        0.45714286]\n",
      "Average F1: [0.33047619 0.39207293 0.50217449]\n",
      "Average recall: [0.2952381  0.42154762 0.5797619 ]\n",
      "============ UJI IMPLEMENTASI CROSS VALIDATION (SKLEARN) =================\n",
      "Confusion Matrix:\n",
      "[[3 0 0]\n",
      " [5 0 0]\n",
      " [7 0 0]]\n",
      "Accuracy: 0.2\n",
      "Precision: [0.2 0.  0. ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1: [0.33333333 0.         0.        ]\n",
      "Confusion Matrix:\n",
      "[[3 0 0]\n",
      " [5 0 0]\n",
      " [7 0 0]]\n",
      "Accuracy: 0.2\n",
      "Precision: [0.2 0.  0. ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1: [0.33333333 0.         0.        ]\n",
      "Confusion Matrix:\n",
      "[[3 0 0]\n",
      " [5 0 0]\n",
      " [7 0 0]]\n",
      "Accuracy: 0.2\n",
      "Precision: [0.2 0.  0. ]\n",
      "Recall: [1. 0. 0.]\n",
      "F1: [0.33333333 0.         0.        ]\n",
      "Confusion Matrix:\n",
      "[[2 1 0]\n",
      " [2 3 0]\n",
      " [1 6 0]]\n",
      "Accuracy: 0.3333333333333333\n",
      "Precision: [0.4 0.3 0. ]\n",
      "Recall: [0.66666667 0.6        0.        ]\n",
      "F1: [0.5 0.4 0. ]\n",
      "Confusion Matrix:\n",
      "[[0 3 0]\n",
      " [0 5 0]\n",
      " [0 7 0]]\n",
      "Accuracy: 0.3333333333333333\n",
      "Precision: [0.         0.33333333 0.        ]\n",
      "Recall: [0. 1. 0.]\n",
      "F1: [0.  0.5 0. ]\n",
      "Confusion Matrix:\n",
      "[[0 3 0]\n",
      " [0 5 0]\n",
      " [0 7 0]]\n",
      "Accuracy: 0.3333333333333333\n",
      "Precision: [0.         0.33333333 0.        ]\n",
      "Recall: [0. 1. 0.]\n",
      "F1: [0.  0.5 0. ]\n",
      "Confusion Matrix:\n",
      "[[0 3 0]\n",
      " [0 3 2]\n",
      " [0 4 3]]\n",
      "Accuracy: 0.4\n",
      "Precision: [0.  0.3 0.6]\n",
      "Recall: [0.         0.6        0.42857143]\n",
      "F1: [0.  0.4 0.5]\n",
      "Confusion Matrix:\n",
      "[[0 0 3]\n",
      " [0 0 5]\n",
      " [0 0 7]]\n",
      "Accuracy: 0.4666666666666667\n",
      "Precision: [0.         0.         0.46666667]\n",
      "Recall: [0. 0. 1.]\n",
      "F1: [0.         0.         0.63636364]\n",
      "Confusion Matrix:\n",
      "[[0 0 3]\n",
      " [0 0 5]\n",
      " [0 0 7]]\n",
      "Accuracy: 0.4666666666666667\n",
      "Precision: [0.         0.         0.46666667]\n",
      "Recall: [0. 0. 1.]\n",
      "F1: [0.         0.         0.63636364]\n",
      "Confusion Matrix:\n",
      "[[0 0 3]\n",
      " [0 0 5]\n",
      " [0 0 7]]\n",
      "Accuracy: 0.4666666666666667\n",
      "Precision: [0.         0.         0.46666667]\n",
      "Recall: [0. 0. 1.]\n",
      "F1: [0.         0.         0.63636364]\n"
     ]
    }
   ],
   "source": [
    "# UJI SKEMA CROSS VALIDATION\n",
    "print(\"============ UJI IMPLEMENTASI CROSS VALIDATION =================\")\n",
    "from sklearn.model_selection import KFold\n",
    "nn = NeuralNetwork(n_layers=3, dataset=dataset, batch_size=50, n_neuron=[3, 2, 5], activation=[\"sigmoid\", \"sigmoid\", \"sigmoid\"])\n",
    "nn.train()\n",
    "# 10 fold implementation\n",
    "nn.cross_validate()\n",
    "\n",
    "# 10 Fold sklearn\n",
    "print(\"============ UJI IMPLEMENTASI CROSS VALIDATION (SKLEARN) =================\")\n",
    "\n",
    "X = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "kf = KFold(n_splits=10)\n",
    "for train_index, test_index in kf.split(X):\n",
    "# ...     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    nn.set_input(X_train)\n",
    "    nn.set_target(y_train)\n",
    "    nn.train()\n",
    "\n",
    "    nn.set_predict(X_test)\n",
    "    nn.prediction(y_test)\n",
    "\n",
    "    summary(confusionMatrix(test.target, nn.output_predict))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7c4dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ UJI SKLEARN CROSS VALIDATION =================\n",
      "Accuracy: 0.853 (0.308)\n"
     ]
    }
   ],
   "source": [
    "print(\"============ UJI SKLEARN CROSS VALIDATION =================\")\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "\n",
    "# Load data\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "\n",
    "\n",
    "train_data = scaler.transform(X)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', hidden_layer_sizes=(3, 2, 5), max_iter=1000, batch_size=2)\n",
    "clf.fit(X, y)   \n",
    "clf.score(X, y)\n",
    "\n",
    "# 10 Cross Validation\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "acc_scores = cross_val_score(clf, X, y, scoring=\"accuracy\", cv=kf)\n",
    "\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(acc_scores), std(acc_scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3764c6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVE MODEL\n",
    "# FULL TRAINING\n",
    "nn = NeuralNetwork(n_layers=3, dataset=dataset, batch_size=50, n_neuron=[3, 2, 5], activation=[\"sigmoid\", \"sigmoid\", \"sigmoid\"])\n",
    "nn.train()\n",
    "nn.save_model(\"model1.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2391c32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL\n",
    "nn.load_model(\"model1_updated_weights.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45245d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PREDIKSI INSTANCE BARU\n",
    "new_data = [1, 3, 4, 5]\n",
    "nn.set_predict([new_data])\n",
    "nn.prediction2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f832ae",
   "metadata": {},
   "source": [
    "## ANALISIS HASIL DARI 2 DAN 3\n",
    "### Hasil 2\n",
    "Pada nomor 2, kami melakukan full training dari dataset menggunakan model hasil implementasi dan model dari library sklearn. Dengan menggunakan beberapa metric uji, diperoleh hasil berikut  \n",
    "  \n",
    "Confusion Matrix:  \n",
    "[[50  0  0]  \n",
    " [ 0 50  0]  \n",
    " [ 0  0 50]]  \n",
    "Accuracy: 1.0  \n",
    "Precision: [1. 1. 1.]  \n",
    "Recall: [1. 1. 1.]  \n",
    "F1: [1. 1. 1.]  \n",
    "  \n",
    "Dapat dilihat hampir seluruh metric memberi skor 1 yang memberikan tanda bahwa model memberikan bobot akhir yang dapat memprediksi masukan training ke kelas target secara benar. Namun setelah melakukan pembelajaran dengan MLP solver \"adam\", diperoleh hasil:  \n",
    "  \n",
    "Confusion Matrix:  \n",
    "[[50  0  0]  \n",
    " [ 0 38 12]  \n",
    " [ 0  0 50]]  \n",
    "Accuracy: 0.92  \n",
    "Precision: [1.         1.         0.80645161]  \n",
    "Recall: [1.   0.76 1.  ]  \n",
    "F1: [1.         0.86363636 0.89285714]  \n",
    "  \n",
    "Berdasarkan hasil berikut, dapat dilihat bahwa pembelajaran yang dilakukan sklearn belum memberikan bobot baru yang optimal untuk melakukan prediksi kelas target.  \n",
    "\n",
    "### Hasil 3\n",
    "Pada nomor 3, \n",
    "============ UJI IMPLEMENTASI SPLIT TEST =================\n",
    "Confusion Matrix:\n",
    "[[6 0 0]\n",
    " [0 5 0]\n",
    " [0 0 4]]\n",
    "Accuracy: 1.0\n",
    "Precision: [1. 1. 1.]\n",
    "Recall: [1. 1. 1.]\n",
    "F1: [1. 1. 1.]\n",
    "============ UJI SKLEARN SPLIT TEST =================\n",
    "[[6 0 0]\n",
    " [0 4 1]\n",
    " [0 0 4]]\n",
    "0.09333333333333334\n",
    "[1.  1.  0.8]\n",
    "[1.  0.8 1. ]\n",
    "[1.         0.88888889 0.88888889]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
