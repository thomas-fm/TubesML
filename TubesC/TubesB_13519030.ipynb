{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7055fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e808489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer0\n",
      "[[ 0.17194334  0.24592462  0.22284241]\n",
      " [ 0.07240757  0.25449259 -0.80984823]\n",
      " [ 1.49673877  0.20589737  0.19795459]\n",
      " [ 1.04438292 -1.20783391  0.38937531]]\n",
      "Layer1\n",
      "[[ 0.67535837  1.38560881]\n",
      " [-1.05930995 -0.81788532]\n",
      " [ 0.63493389  0.49143746]]\n",
      "Layer2\n",
      "[[ 0.94591172  1.73051552  0.02747883 -0.37414413  0.21257408]\n",
      " [ 1.58584852 -0.44684566 -0.44820013  0.85386336 -0.20896722]]\n",
      "Layer3\n",
      "[[-1.14362235 -0.33756202  1.62955367]\n",
      " [-1.44306305  1.1749761  -0.13250071]\n",
      " [ 0.67849192  0.30628169 -0.3995025 ]\n",
      " [ 0.8275892  -0.76692131  0.30628003]\n",
      " [ 0.58543129  0.35526462 -0.42796992]]\n",
      "Jumlah iterasi\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (100) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "iris = load_iris()\n",
    "datasets = train_test_split(iris.data, iris.target,test_size=0.2)\n",
    "train_data, test_data, train_labels, test_labels = datasets\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', hidden_layer_sizes=(3, 2, 5), max_iter=100, batch_size=2)\n",
    "clf.fit(train_data, train_labels)   \n",
    "clf.score(train_data, train_labels)\n",
    "\n",
    "# Bobot layer\n",
    "i = 0\n",
    "for w in clf.coefs_:\n",
    "    print(f\"Layer{i}\")\n",
    "    print(w)\n",
    "    i+=1\n",
    "\n",
    "print(\"Jumlah iterasi\")\n",
    "print(clf.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b54becb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.61390016 0.50904441 0.67476781]]\n",
      "digraph \"Feed Forward Neural Network\" {\n",
      "\tedge [arrowhead=vee arrowsize=2]\n",
      "\tnode [shape=circle width=1.0]\n",
      "\tbx0 -> h1_0 [label=-0.36]\n",
      "\tbx0 -> h1_1 [label=0.35]\n",
      "\tbx0 -> h1_2 [label=0.27]\n",
      "\tx1 -> h1_0 [label=-0.24]\n",
      "\tx1 -> h1_1 [label=-0.00]\n",
      "\tx1 -> h1_2 [label=-0.05]\n",
      "\tx2 -> h1_0 [label=0.15]\n",
      "\tx2 -> h1_1 [label=0.29]\n",
      "\tx2 -> h1_2 [label=-0.40]\n",
      "\tx3 -> h1_0 [label=-0.47]\n",
      "\tx3 -> h1_1 [label=0.34]\n",
      "\tx3 -> h1_2 [label=-0.07]\n",
      "\tx4 -> h1_0 [label=0.26]\n",
      "\tx4 -> h1_1 [label=-0.50]\n",
      "\tx4 -> h1_2 [label=-0.05]\n",
      "\tbh1 -> h2_0 [label=0.22]\n",
      "\tbh1 -> h2_1 [label=-0.27]\n",
      "\th1_0 -> h2_0 [label=0.44]\n",
      "\th1_0 -> h2_1 [label=0.40]\n",
      "\th1_1 -> h2_0 [label=-0.47]\n",
      "\th1_1 -> h2_1 [label=-0.48]\n",
      "\th1_2 -> h2_0 [label=0.04]\n",
      "\th1_2 -> h2_1 [label=0.43]\n",
      "\tbh2 -> h3_0 [label=-0.11]\n",
      "\tbh2 -> h3_1 [label=-0.28]\n",
      "\tbh2 -> h3_2 [label=-0.07]\n",
      "\tbh2 -> h3_3 [label=-0.46]\n",
      "\tbh2 -> h3_4 [label=-0.27]\n",
      "\th2_0 -> h3_0 [label=-0.05]\n",
      "\th2_0 -> h3_1 [label=0.00]\n",
      "\th2_0 -> h3_2 [label=-0.26]\n",
      "\th2_0 -> h3_3 [label=-0.26]\n",
      "\th2_0 -> h3_4 [label=-0.27]\n",
      "\th2_1 -> h3_0 [label=-0.03]\n",
      "\th2_1 -> h3_1 [label=-0.20]\n",
      "\th2_1 -> h3_2 [label=-0.47]\n",
      "\th2_1 -> h3_3 [label=0.35]\n",
      "\th2_1 -> h3_4 [label=0.07]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import seed\n",
    "from random import random, uniform\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import graphviz\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    net_h = np.array(x)\n",
    "    numerator = np.exp(net_h)\n",
    "    denominator = np.sum(np.exp(x))\n",
    "    softmax_output = numerator / denominator\n",
    "    return softmax_output\n",
    "\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s * (1 - s)\n",
    "\n",
    "# todo\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return [0 if (el < 0) else 1 for el in x]\n",
    "\n",
    "# todo\n",
    "def softmax_derivative(x):\n",
    "    return [-(1-el) for el in x]\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_input, n_nodes):\n",
    "        self.weights = []\n",
    "        self.n_input = n_input\n",
    "        self.n_nodes = n_nodes\n",
    "        self.activations = \"\"\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        self.error = []\n",
    "        self.updated_weights = []\n",
    "\n",
    "    def update_weights_back_propagation(self):\n",
    "        self.weights = self.updated_weights.copy()\n",
    "        self.updated_weights = []\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, n_layers, n_neuron=[], activation=[],\n",
    "                 learning_rate=0.1, err_threshold=0.01,\n",
    "                 max_iter=100, batch_size=2, dataset=load_iris(),\n",
    "                 n_input=4, n_output=3):\n",
    "        # Load iris dataset\n",
    "        self.dataset = dataset  # dataset\n",
    "        # self.input = dataset.data  # input\n",
    "        self.target = dataset.target  # target\n",
    "        self.target_names = dataset.target_names  # target class name\n",
    "        self.n_attr = n_input  # n input attribute\n",
    "\n",
    "        # Neural network\n",
    "        self.n_layers = n_layers  # how many hidden layers\n",
    "        self.n_neuron = n_neuron  # how many neuron for each hidden layer\n",
    "        self.activation = activation  # activation for each layer\n",
    "        self.learning_rate = learning_rate\n",
    "        self.err_threshold = err_threshold\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.layers = []\n",
    "        self.bias = 1\n",
    "        self.output = []  # final output from forward propagate\n",
    "\n",
    "        # Back prop\n",
    "        self.error_hidden_value = 0\n",
    "        self.updated_weights = []\n",
    "        self.error = 999  # current error?\n",
    "        self.weights = []  # last updated weight\n",
    "        self.predict = []\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            # n input = n neuron in layer\n",
    "            if i == 0:\n",
    "                layer = Layer(self.n_attr+1, n_neuron[i])\n",
    "                layer.weights = [\n",
    "                    [uniform(-0.5, 0.5) for i in range(n_neuron[i])] for j in range(self.n_attr+1)]\n",
    "            else:\n",
    "                layer = Layer(n_neuron[i-1]+1, n_neuron[i])\n",
    "                layer.weights = [\n",
    "                    [uniform(-0.5, 0.5) for i in range(n_neuron[i])] for j in range(n_neuron[i-1]+1)]\n",
    "            # initalize weight\n",
    "            layer.activations = activation[i]\n",
    "\n",
    "            self.layers.append(layer)\n",
    "\n",
    "        # add last layer, last hidden to output\n",
    "        layer = Layer(n_neuron[-1] + 1, n_output)\n",
    "        layer.weights = [[uniform(-0.5, 0.5) for i in range(n_output)]\n",
    "                         for j in range(n_neuron[-1] + 1)]\n",
    "        layer.activations = activation[-1]\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # todo\n",
    "    def forward_propagation(self, type):\n",
    "        for i in range(self.n_layers + 1):\n",
    "            self.layers[i].input = []\n",
    "        # The first input layer\n",
    "        if type == \"train\":\n",
    "            self.layers[0].input = self.input\n",
    "        elif type == \"predict\":\n",
    "            self.layers[0].input = self.predict\n",
    "\n",
    "        # All hidden layers\n",
    "        for i in range(self.n_layers + 1):\n",
    "            # add bias\n",
    "            bias_input = self.bias\n",
    "            if (i == 0):  # if first hidden layer, convert to array from ndarray and then add bias in the last index\n",
    "                temp_input = []\n",
    "                for j in range(len(self.layers[i].input)):\n",
    "                    input_row = []\n",
    "                    for k in range(len(self.layers[i].input[j])):\n",
    "                        input_row.append(self.layers[i].input[j][k])\n",
    "                    input_row.append(bias_input)\n",
    "                    temp_input.append(input_row)\n",
    "                self.layers[i].input = temp_input\n",
    "            else:  # if not first layer the immediately add the bias in the last index\n",
    "                for j in range(len(self.layers[i].input)):\n",
    "                    self.layers[i].input[j].append(bias_input)\n",
    "\n",
    "            # calculate sigma\n",
    "            self.layers[i].output = np.dot(\n",
    "                self.layers[i].input, self.layers[i].weights)\n",
    "\n",
    "            # activation function\n",
    "            for j in range(len(self.layers[i].output)):\n",
    "                input_next_layer = []  # temporary list to store the next layer's input\n",
    "                for k in range(len(self.layers[i].output[j])):\n",
    "                    x = self.layers[i].output[j][k]\n",
    "                    result = 0\n",
    "                    if (self.layers[i].activations.lower() == \"linear\"):\n",
    "                        result = format(linear(x))\n",
    "                    elif (self.layers[i].activations.lower() == \"sigmoid\"):\n",
    "                        result = format(sigmoid(x))\n",
    "                    elif (self.layers[i].activations.lower() == \"relu\"):\n",
    "                        result = format(relu(x))\n",
    "                    elif (self.layers[i].activations.lower() == \"softmax\"):\n",
    "                        result = format(softmax(x))\n",
    "                    else:  # if activation is not linier, relu, sigmoid, or softmax\n",
    "                        print(\n",
    "                            f\"{self.layers[i].activations}: Invalid activation method!\")\n",
    "                        return\n",
    "\n",
    "                    # append output, actually layers[i].output == layers[i+1].input\n",
    "                    self.layers[i].output[j][k] = result\n",
    "                    # append input for next layer in temporary list (input_next_layer)\n",
    "                    input_next_layer.append(float(result))\n",
    "\n",
    "                if (i < self.n_layers):  # if there is still the next layer\n",
    "                    # append input for next layer in layers[i+1].input\n",
    "                    self.layers[i+1].input.append(input_next_layer)\n",
    "\n",
    "        # output in the last layer\n",
    "        self.output = self.layers[-1].output.copy()\n",
    "\n",
    "    # todo\n",
    "    def error_output(self):\n",
    "        # get output layer\n",
    "        output_layer = self.layers[self.n_layers]\n",
    "        activation_rule = output_layer.activations.lower()\n",
    "        output_layer.error = output_layer.output.copy()\n",
    "        for i in range(len(self.input)):\n",
    "            expected_target = []\n",
    "            if (self.target[i] == 0):\n",
    "                expected_target = [1, 0, 0]\n",
    "            if (self.target[i] == 1):\n",
    "                expected_target = [0, 1, 0]\n",
    "            if (self.target[i] == 2):\n",
    "                expected_target = [0, 0, 1]\n",
    "            for j in range(3):\n",
    "                output_layer.error[i][j] = expected_target[j] - output_layer.error[i][j]\n",
    "        self.error = np.mean(output_layer.error)\n",
    "\n",
    "        # calculate error per output node\n",
    "        for i in range(len(output_layer.error)):\n",
    "            if (activation_rule == \"sigmoid\"):\n",
    "                output_layer.error[i] *= sigmoid_derivative(\n",
    "                    output_layer.output[i])\n",
    "            elif (activation_rule == \"relu\"):\n",
    "                output_layer.error[i] *= relu_derivative(\n",
    "                    output_layer.output[i])\n",
    "            elif (activation_rule == \"linear\"):\n",
    "                output_layer.error[i] *= linear_derivative(\n",
    "                    output_layer.output[i])\n",
    "            elif (activation_rule == \"softmax\"):\n",
    "                output_layer.error[i] *= softmax_derivative(\n",
    "                    output_layer.output[i])\n",
    "            \n",
    "        return\n",
    "\n",
    "    # todo\n",
    "    def error_hidden(self):\n",
    "        #menghitung delta net j = delta k * weight j k\n",
    "        delta_net_j = 0\n",
    "        for i in range(self.n_layers-1, -1, -1):\n",
    "            delta_net_j_array = []\n",
    "            for j in range(len(self.layers[i+1].error)):\n",
    "                delta_net_j_data = []\n",
    "                for k in range(len(self.layers[i+1].weights)-1):\n",
    "                    # print(self.layers[i+1].error)\n",
    "                    delta_net_j = np.dot(self.layers[i+1].error[j], np.transpose(self.layers[i+1].weights[k]))\n",
    "                    delta_net_j_data.append(delta_net_j)\n",
    "                    \n",
    "\n",
    "                delta_net_j_array.append(delta_net_j_data)\n",
    "                \n",
    "            self.layers[i].error = self.layers[i].output.copy()\n",
    "\n",
    "            #menghitung error di hidden layer\n",
    "            for j in range(len(self.layers[i].error)):\n",
    "                if (self.layers[i].activations == \"sigmoid\"):\n",
    "                    self.layers[i].error[j] = np.dot(sigmoid_derivative(\n",
    "                        self.layers[i].output[j]),  delta_net_j_array[j])\n",
    "                elif (self.layers[i].activations == \"relu\"):\n",
    "                    self.layers[i].error[j] = np.dot(relu_derivative(\n",
    "                        self.layers[i].output[j]), delta_net_j_array[j])\n",
    "                elif (self.layers[i].activations == \"linear\"):\n",
    "                    self.layers[i].error[j] = np.dot(linear_derivative(\n",
    "                        self.layers[i].output[j]), delta_net_j_array[j])\n",
    "        \n",
    "        return\n",
    "\n",
    "    # todo\n",
    "    def update_weights(self, row_input, old_weight, error_term):\n",
    "        new_weight_list = []\n",
    "\n",
    "        # new weight for hidden layers\n",
    "        for i in range(len(row_input)):\n",
    "            new_weight = old_weight[i] - self.learning_rate * error_term * row_input[i]\n",
    "            new_weight_list.append(new_weight)\n",
    "        \n",
    "        return new_weight_list\n",
    "\n",
    "    # todo\n",
    "    def back_propagation(self):\n",
    "        # output layer\n",
    "        self.error_output()\n",
    "        output_layer = self.layers[self.n_layers]\n",
    "        self.error = np.mean(output_layer.error)\n",
    "        updated_weights_temp = []\n",
    "\n",
    "        for i in range(len(output_layer.output)):\n",
    "            updated_weights_temp.append(self.update_weights(\n",
    "                output_layer.input[i], output_layer.weights, output_layer.error[i]))\n",
    "        output_layer.updated_weights = output_layer.weights.copy()\n",
    "        output_layer.updated_weights = np.mean(\n",
    "            updated_weights_temp, axis=0)\n",
    "        \n",
    "\n",
    "        self.error_hidden()\n",
    "        for i in range(self.n_layers-1, -1 ,-1):\n",
    "            self.error_hidden_value = np.mean(self.layers[i].error)\n",
    "\n",
    "            updated_weights_temp_hidden = []\n",
    "            for j in range(len(self.layers[i].input)):\n",
    "                updated_weights_temp_hidden.append(self.update_weights(self.layers[i].input[j], self.layers[i].weights, self.layers[i].error[j]))\n",
    "            self.layers[i].updated_weights = self.layers[i].weights.copy()\n",
    "            self.layers[i].updated_weights = np.mean(updated_weights_temp_hidden, axis=0)\n",
    "\n",
    "        # update all weights\n",
    "        for layer in self.layers:\n",
    "            layer.update_weights_back_propagation()\n",
    "        return\n",
    "\n",
    "    # todo\n",
    "    def train(self):\n",
    "        it = 0\n",
    "        while ((it < self.max_iter) and (self.err_threshold < self.error)):\n",
    "            output = []\n",
    "            error = []\n",
    "            # random.shuffle(self.datasets.data)\n",
    "            for i in range(int(len(self.dataset.data)/self.batch_size)):\n",
    "                idx = i * self.batch_size\n",
    "                self.input = self.dataset.data[idx:idx+self.batch_size]\n",
    "                self.forward_propagation(type=\"train\")\n",
    "                self.back_propagation()\n",
    "                output.append(self.layers[self.n_layers].output)\n",
    "                error.append(self.layers[self.n_layers].error)\n",
    "                \n",
    "            it += 1\n",
    "\n",
    "        return\n",
    "\n",
    "    def set_predict(self, input):\n",
    "        self.predict = input\n",
    "\n",
    "    def prediction(self):\n",
    "        self.forward_propagation(type=\"predict\")\n",
    "        print(self.output)\n",
    "        # return self.output\n",
    "    def check_sanity(self):\n",
    "        for layer in self.layers:\n",
    "            print(layer.weights)\n",
    "    \n",
    "    def draw_model(self):\n",
    "        f = graphviz.Digraph('Feed Forward Neural Network', filename=\"model\")\n",
    "        f.attr('node', shape='circle', width='1.0')\n",
    "        f.edge_attr.update(arrowhead='vee', arrowsize='2')\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            if i == 0:\n",
    "                for j in range(len(self.layers[i].weights)): #count weights\n",
    "                    for k in range(len(self.layers[i].weights[j])): #output node\n",
    "                        if j==0:\n",
    "                            f.edge(f\"bx{j}\", f\"h{i+1}_{k}\", \n",
    "                                   f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "                        else:\n",
    "                            f.edge(f\"x{j}\", f\"h{i+1}_{k}\", \n",
    "                                   f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "            else:\n",
    "                for j in range(len(self.layers[i].weights)): #count weights\n",
    "                    for k in range(len(self.layers[i].weights[j])): #output node\n",
    "                        if j==0:\n",
    "                            f.edge(f\"bh{i}\", f\"h{i+1}_{k}\", \n",
    "                                   f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "                        else:\n",
    "                            f.edge(f\"h{i}_{j-1}\", f\"h{i+1}_{k}\", \n",
    "                                   f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "        \n",
    "        print(f.source)\n",
    "        f.render(directory='model').replace('\\\\', '/')\n",
    "\n",
    "seed(1)\n",
    "\n",
    "# Normalize data\n",
    "dataset = load_iris()\n",
    "\n",
    "nn = NeuralNetwork(n_layers=3, dataset=dataset, batch_size=50, n_neuron=[3, 2, 5], activation=[\"sigmoid\", \"sigmoid\", \"sigmoid\"])\n",
    "# nn.forward_propagation()\n",
    "nn.train()\n",
    "nn.set_predict([[1.0, 6.5, 2.0, 3.5]])\n",
    "nn.prediction()\n",
    "nn.draw_model()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
