{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer0\n",
      "[[-0.41737118  0.17404616 -0.48244752]\n",
      " [ 0.3110146   0.77892937 -0.1932483 ]\n",
      " [ 0.85463744  0.31707493  0.24566981]\n",
      " [-0.21687531  0.68307672  0.08779393]]\n",
      "Layer1\n",
      "[[-0.79354462 -0.84734224]\n",
      " [-0.77280644 -0.32772938]\n",
      " [-1.02717823 -0.94194244]]\n",
      "Layer2\n",
      "[[ 0.74111614 -0.18249472 -0.88929958 -0.68658184  0.89363164]\n",
      " [-0.73511016 -0.21158881 -0.73578539 -0.02444385 -0.92004767]]\n",
      "Layer3\n",
      "[[-0.1051237  -0.04857853  0.09302841]\n",
      " [-0.15675989  0.06870962  0.2861304 ]\n",
      " [ 0.02274889  0.18529515 -0.81486312]\n",
      " [ 0.75246826  0.28494133  0.43414881]\n",
      " [-0.49399612 -0.75117137 -0.12587558]]\n",
      "Jumlah iterasi\n",
      "42\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "iris = load_iris()\n",
    "datasets = train_test_split(iris.data, iris.target,test_size=0.2)\n",
    "train_data, test_data, train_labels, test_labels = datasets\n",
    "\n",
    "# Normalize\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data)\n",
    "\n",
    "train_data = scaler.transform(train_data)\n",
    "test_data = scaler.transform(test_data)\n",
    "\n",
    "clf = MLPClassifier(solver='sgd', hidden_layer_sizes=(3, 2, 5), max_iter=100, batch_size=2)\n",
    "clf.fit(train_data, train_labels)   \n",
    "clf.score(train_data, train_labels)\n",
    "\n",
    "# Bobot layer\n",
    "i = 0\n",
    "for w in clf.coefs_:\n",
    "    print(f\"Layer{i}\")\n",
    "    print(w)\n",
    "    i+=1\n",
    "\n",
    "print(\"Jumlah iterasi\")\n",
    "print(clf.n_iter_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last output : [[4.53978687e-05 9.35762297e-14 9.99954602e-01]\n",
      " [4.53978687e-05 9.35762297e-14 9.99954602e-01]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ad784e7bfff5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"model1.txt\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;31m# nn.forward_propagation()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;31m# nn.set_predict([[1.0, 6.5, 2.0, 3.5]])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[1;31m# nn.prediction()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ad784e7bfff5>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ad784e7bfff5>\u001b[0m in \u001b[0;36mback_propagation\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mback_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    252\u001b[0m         \u001b[1;31m# output layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 253\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    254\u001b[0m         \u001b[0moutput_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-ad784e7bfff5>\u001b[0m in \u001b[0;36merror_output\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0merror_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m         \u001b[1;31m# get output layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0moutput_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[0mactivation_rule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0moutput_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from random import seed\n",
    "from random import random, uniform\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import graphviz\n",
    "\n",
    "def linear(x):\n",
    "    return x\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    net_h = np.array(x)\n",
    "    numerator = np.exp(net_h)\n",
    "    denominator = np.sum(np.exp(x))\n",
    "    softmax_output = numerator / denominator\n",
    "    return softmax_output\n",
    "\n",
    "\n",
    "def linear_derivative(x):\n",
    "    return 1\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s * (1 - s)\n",
    "\n",
    "# todo\n",
    "\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return [0 if (el < 0) else 1 for el in x]\n",
    "\n",
    "# todo\n",
    "def softmax_derivative(x):\n",
    "    return [-(1-el) for el in x]\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, n_input, n_nodes):\n",
    "        self.weights = []\n",
    "        self.n_input = n_input\n",
    "        self.n_nodes = n_nodes\n",
    "        self.activations = \"\"\n",
    "        self.input = []\n",
    "        self.output = []\n",
    "        self.error = []\n",
    "        self.updated_weights = []\n",
    "\n",
    "    def update_weights_back_propagation(self):\n",
    "        self.weights = self.updated_weights.copy()\n",
    "        self.updated_weights = []\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, file_name, learning_rate=0.1, err_threshold=0.01, max_iter=100, batch_size=2, dataset=load_iris(), n_input=4, n_output=3):\n",
    "        # Load iris dataset\n",
    "        self.dataset = dataset  # dataset\n",
    "        # self.input = dataset.data  # input\n",
    "        self.target = dataset.target  # target\n",
    "        self.target_names = dataset.target_names  # target class name\n",
    "        self.n_attr = n_input  # n input attribute\n",
    "\n",
    "        # Neural network\n",
    "        self.learning_rate = learning_rate\n",
    "        self.err_threshold = err_threshold\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.layers = []\n",
    "        self.bias = 1\n",
    "        self.output = []  # final output from forward propagate\n",
    "\n",
    "        # Back prop\n",
    "        self.error_hidden_value = 0\n",
    "        self.updated_weights = []\n",
    "        self.error = 999  # current error?\n",
    "        self.weights = []  # last updated weight\n",
    "        self.predict = []\n",
    "        \n",
    "        with open(file_name, \"r\") as f:\n",
    "            line = f.readline().split()\n",
    "            self.n_layers = len(line)  # how many hidden layers + ouput\n",
    "            \n",
    "            for i in range(self.n_layers):\n",
    "                if i == 0:\n",
    "                    self.layers.append(Layer(self.n_attr, int(line[i])))\n",
    "                else:\n",
    "                    self.layers.append(Layer(int(line[i-1]), int(line[i])))\n",
    "\n",
    "            for i in range(self.n_layers):\n",
    "                f.readline()\n",
    "                for j in range(self.layers[i].n_input + 2):\n",
    "                    weight = []\n",
    "                    line = f.readline().strip(\" \\n\").split(\" \")\n",
    "                    for k in range(len(line)):\n",
    "                        if (j == 0):\n",
    "                            self.layers[i].activations = str(line[k])\n",
    "                        else:\n",
    "                            weight.append(float(line[k]))\n",
    "                    if j!=0:\n",
    "                        self.layers[i].weights.append(weight)\n",
    "                        \n",
    "    def forward_propagation(self, type):\n",
    "        for i in range(self.n_layers):\n",
    "            self.layers[i].input = []\n",
    "        # The first input layer\n",
    "        if type == \"train\":\n",
    "            self.layers[0].input = self.input\n",
    "        elif type == \"predict\":\n",
    "            self.layers[0].input = self.predict\n",
    "\n",
    "        # All hidden layers\n",
    "        for i in range(self.n_layers):\n",
    "            # add bias\n",
    "            bias_input = self.bias\n",
    "            if (i == 0):  # if first hidden layer, convert to array from ndarray and then add bias in the last index\n",
    "                temp_input = []\n",
    "                for j in range(len(self.layers[i].input)):\n",
    "                    input_row = []\n",
    "                    for k in range(len(self.layers[i].input[j])):\n",
    "                        input_row.append(self.layers[i].input[j][k])\n",
    "                    input_row.insert(0, (bias_input))\n",
    "                    temp_input.append(input_row)\n",
    "                self.layers[i].input = temp_input\n",
    "            else:  # if not first layer the immediately add the bias in the last index\n",
    "                for j in range(len(self.layers[i].input)):\n",
    "                    self.layers[i].input[j].insert(0, (bias_input))\n",
    "\n",
    "            # calculate sigma\n",
    "            self.layers[i].output = np.dot(self.layers[i].input, self.layers[i].weights)\n",
    "                \n",
    "            # activation function\n",
    "            for j in range(len(self.layers[i].output)):\n",
    "                input_next_layer = []  # temporary list to store the next layer's input\n",
    "                for k in range(len(self.layers[i].output[j])):\n",
    "                    x = self.layers[i].output[j][k]\n",
    "                    result = 0\n",
    "                    if (self.layers[i].activations.lower() == \"linear\"):\n",
    "                        result = format(linear(x))\n",
    "                    elif (self.layers[i].activations.lower() == \"sigmoid\"):\n",
    "                        result = format(sigmoid(x))\n",
    "                    elif (self.layers[i].activations.lower() == \"relu\"):\n",
    "                        result = format(relu(x))\n",
    "                    elif (self.layers[i].activations.lower() == \"softmax\"):\n",
    "                        result = format(softmax(x))\n",
    "                    else:  # if activation is not linier, relu, sigmoid, or softmax\n",
    "                        print(\n",
    "                            f\"{self.layers[i].activations}: Invalid activation method!\")\n",
    "                        return\n",
    "\n",
    "                    # append output, actually layers[i].output == layers[i+1].input\n",
    "                    self.layers[i].output[j][k] = result\n",
    "                    # append input for next layer in temporary list (input_next_layer)\n",
    "                    input_next_layer.append(float(result))\n",
    "\n",
    "                if (i < self.n_layers - 1): # if there is still the next layer\n",
    "                    # append input for next layer in layers[i+1].input\n",
    "                    self.layers[i+1].input.append(input_next_layer)\n",
    "\n",
    "        # output in the last layer\n",
    "        self.output = self.layers[-1].output.copy()\n",
    "        print(f\"last output : {self.output}\")\n",
    "\n",
    "    # todo\n",
    "    def error_output(self):\n",
    "        # get output layer\n",
    "        output_layer = self.layers[self.n_layers]\n",
    "        activation_rule = output_layer.activations.lower()\n",
    "        output_layer.error = output_layer.output.copy()\n",
    "        for i in range(len(self.input)):\n",
    "            expected_target = []\n",
    "            if (self.target[i] == 0):\n",
    "                expected_target = [1, 0, 0]\n",
    "            if (self.target[i] == 1):\n",
    "                expected_target = [0, 1, 0]\n",
    "            if (self.target[i] == 2):\n",
    "                expected_target = [0, 0, 1]\n",
    "            for j in range(3):\n",
    "                output_layer.error[i][j] = expected_target[j] - output_layer.error[i][j]\n",
    "        self.error = np.mean(output_layer.error)\n",
    "\n",
    "        # calculate error per output node\n",
    "        for i in range(len(output_layer.error)):\n",
    "            if (activation_rule == \"sigmoid\"):\n",
    "                output_layer.error[i] *= sigmoid_derivative(\n",
    "                    output_layer.output[i])\n",
    "            elif (activation_rule == \"relu\"):\n",
    "                output_layer.error[i] *= relu_derivative(\n",
    "                    output_layer.output[i])\n",
    "            elif (activation_rule == \"linear\"):\n",
    "                output_layer.error[i] *= linear_derivative(\n",
    "                    output_layer.output[i])\n",
    "            elif (activation_rule == \"softmax\"):\n",
    "                output_layer.error[i] *= softmax_derivative(\n",
    "                    output_layer.output[i])\n",
    "            \n",
    "        return\n",
    "\n",
    "    # todo\n",
    "    def error_hidden(self):\n",
    "        #menghitung delta net j = delta k * weight j k\n",
    "        delta_net_j = 0\n",
    "        for i in range(self.n_layers-1, -1, -1):\n",
    "            delta_net_j_array = []\n",
    "            for j in range(len(self.layers[i+1].error)):\n",
    "                delta_net_j_data = []\n",
    "                for k in range(len(self.layers[i+1].weights)-1):\n",
    "                    # print(self.layers[i+1].error)\n",
    "                    delta_net_j = np.dot(self.layers[i+1].error[j], np.transpose(self.layers[i+1].weights[k]))\n",
    "                    delta_net_j_data.append(delta_net_j)\n",
    "                    \n",
    "\n",
    "                delta_net_j_array.append(delta_net_j_data)\n",
    "                \n",
    "            self.layers[i].error = self.layers[i].output.copy()\n",
    "\n",
    "            #menghitung error di hidden layer\n",
    "            for j in range(len(self.layers[i].error)):\n",
    "                if (self.layers[i].activations == \"sigmoid\"):\n",
    "                    self.layers[i].error[j] = np.dot(sigmoid_derivative(\n",
    "                        self.layers[i].output[j]),  delta_net_j_array[j])\n",
    "                elif (self.layers[i].activations == \"relu\"):\n",
    "                    self.layers[i].error[j] = np.dot(relu_derivative(\n",
    "                        self.layers[i].output[j]), delta_net_j_array[j])\n",
    "                elif (self.layers[i].activations == \"linear\"):\n",
    "                    self.layers[i].error[j] = np.dot(linear_derivative(\n",
    "                        self.layers[i].output[j]), delta_net_j_array[j])\n",
    "        \n",
    "        return\n",
    "\n",
    "    # todo\n",
    "    def update_weights(self, row_input, old_weight, error_term):\n",
    "        new_weight_list = []\n",
    "\n",
    "        # new weight for hidden layers\n",
    "        for i in range(len(row_input)):\n",
    "            new_weight = old_weight[i] - self.learning_rate * error_term * row_input[i]\n",
    "            new_weight_list.append(new_weight)\n",
    "        \n",
    "        return new_weight_list\n",
    "\n",
    "    # todo\n",
    "    def back_propagation(self):\n",
    "        # output layer\n",
    "        self.error_output()\n",
    "        output_layer = self.layers[self.n_layers]\n",
    "        self.error = np.mean(output_layer.error)\n",
    "        updated_weights_temp = []\n",
    "\n",
    "        for i in range(len(output_layer.output)):\n",
    "            updated_weights_temp.append(self.update_weights(\n",
    "                output_layer.input[i], output_layer.weights, output_layer.error[i]))\n",
    "        output_layer.updated_weights = output_layer.weights.copy()\n",
    "        output_layer.updated_weights = np.mean(\n",
    "            updated_weights_temp, axis=0)\n",
    "        \n",
    "\n",
    "        self.error_hidden()\n",
    "        for i in range(self.n_layers-1, -1 ,-1):\n",
    "            self.error_hidden_value = np.mean(self.layers[i].error)\n",
    "\n",
    "            updated_weights_temp_hidden = []\n",
    "            for j in range(len(self.layers[i].input)):\n",
    "                updated_weights_temp_hidden.append(self.update_weights(self.layers[i].input[j], self.layers[i].weights, self.layers[i].error[j]))\n",
    "            self.layers[i].updated_weights = self.layers[i].weights.copy()\n",
    "            self.layers[i].updated_weights = np.mean(updated_weights_temp_hidden, axis=0)\n",
    "\n",
    "        # update all weights\n",
    "        for layer in self.layers:\n",
    "            layer.update_weights_back_propagation()\n",
    "        return\n",
    "\n",
    "    # todo\n",
    "    def train(self):\n",
    "        it = 0\n",
    "        while ((it < self.max_iter) and (self.err_threshold < self.error)):\n",
    "            output = []\n",
    "            error = []\n",
    "            # random.shuffle(self.datasets.data)\n",
    "            for i in range(int(len(self.dataset.data)/self.batch_size)):\n",
    "                idx = i * self.batch_size\n",
    "                self.input = self.dataset.data[idx:idx+self.batch_size]\n",
    "                self.forward_propagation(type=\"train\")\n",
    "                self.back_propagation()\n",
    "                output.append(self.layers[self.n_layers].output)\n",
    "                error.append(self.layers[self.n_layers].error)\n",
    "                \n",
    "            it += 1\n",
    "\n",
    "        return\n",
    "\n",
    "    def set_predict(self, input):\n",
    "        self.predict = input\n",
    "\n",
    "    def prediction(self):\n",
    "        self.forward_propagation(type=\"predict\")\n",
    "        print(self.output)\n",
    "        # return self.output\n",
    "    def check_sanity(self):\n",
    "        for layer in self.layers:\n",
    "            print(layer.weights)\n",
    "    \n",
    "    def draw_model(self):\n",
    "        f = graphviz.Digraph('Feed Forward Neural Network', filename=\"model\")\n",
    "        f.attr('node', shape='circle', width='1.0')\n",
    "        f.edge_attr.update(arrowhead='vee', arrowsize='2')\n",
    "        \n",
    "        for i in range(self.n_layers):\n",
    "            if i == 0:\n",
    "                for j in range(len(self.layers[i].weights)): #count weights\n",
    "                    for k in range(len(self.layers[i].weights[j])): #output node\n",
    "                        if j==0:\n",
    "                            f.edge(f\"bx{j}\", f\"h{i+1}_{k}\", \n",
    "                                   f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "                        else:\n",
    "                            f.edge(f\"x{j}\", f\"h{i+1}_{k}\", \n",
    "                                   f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "            else:\n",
    "                for j in range(len(self.layers[i].weights)): #count weights\n",
    "                    for k in range(len(self.layers[i].weights[j])): #output node\n",
    "                        if j==0:\n",
    "                            f.edge(f\"bh{i}\", f\"h{i+1}_{k}\", \n",
    "                                   f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "                        else:\n",
    "                            f.edge(f\"h{i}_{j-1}\", f\"h{i+1}_{k}\", \n",
    "                                   f\"{self.layers[i].weights[j][k]:.2f}\")\n",
    "        \n",
    "        print(f.source)\n",
    "#         f.render(directory='model').replace('\\\\', '/')\n",
    "\n",
    "    def save_model(self, filename):\n",
    "        new_file = []\n",
    "        with open(filename) as file:\n",
    "            lines = [line.rstrip().split() for line in file]\n",
    "\n",
    "            new_file.append(lines[0])\n",
    "            new_file.append(lines[1])\n",
    "            \n",
    "            for i in range(self.n_layers):\n",
    "                new_file.append([self.layers[i].activations])\n",
    "                for new_weight in self.layers[i].updated_weights:\n",
    "                    new_file.append(new_weight)\n",
    "                if i < len(b) - 1:\n",
    "                    new_file.append('')\n",
    "                    \n",
    "            for line in range(len(new_file)):\n",
    "                str_line = ''\n",
    "                for i in range(len(new_file[line])):\n",
    "                    str_line += str(new_file[line][i])\n",
    "                    if i < len(new_file[line]) - 1:\n",
    "                        str_line += ' '\n",
    "                new_file[line] = str_line\n",
    "                \n",
    "        new_filename = filename.split(\".\")[0] + \"_updated_weights\"\n",
    "        with open('model/' + new_filename, 'w') as f:\n",
    "            for line in range(len(new_file)):\n",
    "                f.write(new_file[line])\n",
    "                if line < len(new_file) - 1:\n",
    "                    f.write('\\n')\n",
    "\n",
    "seed(1)\n",
    "\n",
    "# Normalize data\n",
    "dataset = load_iris()\n",
    "\n",
    "nn = NeuralNetwork(file_name=\"model1.txt\",dataset=dataset, batch_size=2)\n",
    "# nn.forward_propagation()\n",
    "nn.train()\n",
    "# nn.set_predict([[1.0, 6.5, 2.0, 3.5]])\n",
    "# nn.prediction()\n",
    "# nn.draw_model()\n",
    "nn.save_model(\"model1.txt\")\n",
    "\n",
    "# ff = NeuralNetwork(\"model1.txt\")\n",
    "# ff.forward_propagation(\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
